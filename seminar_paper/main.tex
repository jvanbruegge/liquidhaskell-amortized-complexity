\documentclass[acmlarge,screen,authorversion=true,nonacm=true]{acmart}
%\documentclass[sigplan,screen,review,anonymous]{acmart}

\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}

\usepackage{cancel}

\usepackage{listings}
\lstset{%
    language=Haskell,
    columns=fullflexible,
    basicstyle=\ttfamily,
    commentstyle=\color{codegreen},
    showstringspaces=false
}

% shorthand for big O
\renewcommand\O[1]{$\mathcal{O}(#1)$}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2021}
\acmYear{2021}
\acmDOI{tbd}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Haskell '21]{the 14th ACM SIGPLAN International Haskell Symposium}{August 26--27, 2021}{Virtual Event}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Theorem Proving for All: Equational Reasoning in Liquid Haskell (Seminar paper)}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Jan van Brügge}
\orcid{0000-0003-1560-7326}
\affiliation{%
  \institution{Technical University of Munich}
  \streetaddress{Arcisstraße 21}
  \city{Munich}
  \state{Bavaria}
  \country{Germany}
  \postcode{80333}
}
\email{jan.van-bruegge@tum.de}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
When programming in a pure functional language like Haskell, it is possible to reason about the execution of a function by viewing its definitions as equations that can be used to repeatedly rewrite a given expression until a final result is computed. Usually such reasoning is done "out of band" with pen and paper. LiquidHaskell is a refinement type checker for Haskell that allows the user to do verified equational reasoning directly in Haskell. This seminar paper presents how such reasoning can be used to prove theorems and derive efficient implementations from simple specification. It is based on the functional pearl by Vazou et al.~\cite{tpfa}.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10010940.10010992.10010998.10010999</concept_id>
       <concept_desc>Software and its engineering~Software verification</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software verification}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{LiquidHaskell, theorem proving, seminar paper}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}\label{sec:introduction}

One of the most praised advantages of pure functional languages like Haskell is the possibility to use equational reasoning to make sense of the execution of a function. For a new learner it is helpful to "execute" a given function call manually to understand how the code behaves. For example when given the recursive definition of a very simple factorial function:

\begin{lstlisting}
fact :: Int -> Int
fact 0 = 1
fact n = n * fact (n - 1)
\end{lstlisting}

To understand how this function works, a learner can repeatedly replace the calls to the \texttt{fact} function with its definition and simplify the result:

\begin{lstlisting}
let x = fact 3           -- use definition
      = 3 * fact (3 - 1) -- simplify
      = 3 * fact 2       -- use definition
      = 3 * (2 * fact (2 - 1)) -- simplify
      = 3 * (2 * fact 1) -- use definition
      = 3 * (2 * (1 * fact (1 - 1))) -- simplify
      = 3 * (2 * (1 * fact 0)) -- use definition
      = 3 * (2 * (1 * 1)) -- simplify
      = 3 * (2 * 1) -- simplify
      = 3 * 2 -- simplify
      = 6
\end{lstlisting}

Usually such reasoning is done "out of band", for example with pen and paper. LiquidHaskell~\cite{liquidhaskell} enables the user to do such reasoning and proof properties about Haskell code directly in Haskell itself.

Usually proofs of software are done in external theorem provers like Coq~\cite{coq}, Lean~\cite{lean} or Isabelle/HOL~\cite{isabelle}. But to prove properties about Haskell code, the user first has to translate the code into the language of the respective theorem prover. Such a translation step is not needed with LiquidHaskell. In addition, the user does not need to learn a completely new language with subtly different semantics (for example lazy vs. strict evaluation) to do their proofs.

In this paper we:
\begin{itemize}
\item{list and explain the core features of LiquidHaskell (section~\ref{sec:features})}
\item{present two case studies, one to derive an efficient implementation and one to prove the amortized complexity of a stack (section~\ref{sec:case_studies})}
\item{briefly present the SMT solver that is used to check the refinements (section~\ref{sec:hood})}
\item{list the limitations of LiquidHaskell at the time of writing (section~\ref{sec:limitations})}
\item{compare LiquidHaskell to other popular theorem provers (section~\ref{sec:comparison})}
\item{summarize the current user experience in a conclusion (section~\ref{sec:conclusion})}
\end{itemize}

\section{Core features of LiquidHaskell}\label{sec:features}

\subsection{Refinement types}\label{sec:refinement_types}

The core addition of LiquidHaskell to Haskell's type system are \textit{refinement types}~\cite{refinement_types}. A refinement type allows the user to specify subsets of types to be used during type checking. For example if \texttt{Integer} is the type of all integers, then the refinement type \texttt{\{ x:Integer | x >= 0 \}} specifies the subset of non-negative integers. This syntax is taken directly from LiquidHaskell. Refinement types are written inside of braces (\texttt{\{\}}) where the type on the left of the pipe character (\texttt{|}) can name the type (\texttt{x} in our case) and refer to it on the right. When used on the arguments and return types of functions, refinement types work as pre- and postconditions. This means that LiquidHaskell will check the postcondition (the refinement on the return type) under the assumption of the pre-conditions (the refinements on the function arguments). For example a function that returns the tail of a list for non-empty lists:

\begin{lstlisting}
{-@ tail :: { xs:[a] | len xs > 0 } -> { ys:[a] | len ys == len xs - 1 } @-}
tail :: [a] -> [a]
tail (_:xs) = xs
\end{lstlisting}

As seen above, LiquidHaskell annotations are written in special comments (\texttt{\{-@ @-\}}) so that the code still compiles even without LiquidHaskell. While from the normal Haskell type signature below the annotation the function would be partial (it does not have an equation for the empty list), LiquidHaskell makes use of the precondition and thus can deduce that the input list will never be empty (see more about totality in section~\ref{sec:totality}).

The refinement types can also be extracted into type synonyms to enable reuse of common refinements:

\begin{lstlisting}
{-@ type Nat = { x:Integer | x >= 0 } @-}
\end{lstlisting}

Another property that the specification of subsets on types causes is subtyping. Normally, Haskell has no subtyping like object oriented languages where for example every valid \texttt{Dog} object is also a valid \texttt{Animal} object. With LiquidHaskell, subtyping is defined as the subset relation. For example the earlier type of non-negative integers is a subtype of all integers, because the set of all non-negative integers is a subset of all integers. In practice, this means that the user can pass a value that is a non-negative integer to a function that expects any integer without any explicit type coercions.

\subsection{Totality and termination}\label{sec:totality}

By default all functions in LiquidHaskell need to be total. This means that a function needs to handle all possible inputs and it is not allowed to use the \texttt{error} function on a live code path. While checking totality, LiquidHaskell takes the preconditions into account (see the code example in section~\ref{sec:refinement_types}).

In addition to handling all inputs, a function also has to terminate. For this, LiquidHaskell has a termination checker that infers a suitable termination metric by default. If this is not possible, the user can also specify a termination metric manually. This, however, is rarely needed, because the common case of recursive calls with strictly smaller arguments is usually enough:

\begin{lstlisting}
{-@ length :: [a] -> Int @-}
length :: [a] -> Int
length [] = 0 -- no recursion, terminates trivially
length (x:xs) = 1 + length xs -- argument to recursive call is smaller
\end{lstlisting}

The termination metric allows LiquidHaskell to generate an induction principle for the function (well-founded induction), which is used to prove the postconditions of recursive functions, for example that the sum of a list of non-negative numbers still is non-negative:

\begin{lstlisting}
{-@ sum :: [Nat] -> Nat @-}
sum [] = 0 -- trivial, 0 >= 0
sum (x:xs) =
    x -- from precondition: x >= 0
    + sum xs -- induction, assume postcondition already holds for `sum xs`
\end{lstlisting}

See the case studies in sections~\ref{sec:efficient} and~\ref{sec:complexity} for examples of using these induction principles to prove properties about the code.

\subsection{Lightweight and deep reasoning}\label{sec:reasoning}

By default, the SMT solver powering LiquidHaskell can only reason about linear arithmetic, like in the \texttt{sum} example in section~\ref{sec:totality} above. If the user wants to use functions in the refinements itself, they need to \textit{reflect} them first to the logic level. For a limited class of functions called \textit{measures} this is possible while retaining the complete automatic reasoning of the SMT solver. This is called \textit{lightweight reasoning}. For all other functions, the user needs to provide the proofs themselves (\textit{deep reasoning}). A measure is a function that:
\begin{itemize}
\item{takes exactly one argument, which must be an algebraic data type}
\item{is defined by a single equation per constructor of the data type}
\item{only calls primitive (e.g. arithmetic) functions of the SMT solver and other measures in its body}
\item{is marked with the \texttt{\{-@ measure @-\}} pragma}
\end{itemize}

As an example, the \texttt{length} function from earlier is a measure:

\begin{lstlisting}
{-@ measure length @-}
{-@ length :: [a] -> Nat @-}
length :: [a] -> Int
length [] = 0 -- one equation for [] constructor
length (x:xs) = 1 + length xs -- one equation for (:) constructor
\end{lstlisting}

Because we retain the automated reasoning for \texttt{length} we can use it in refinement without specifying a proof manually, for example to define an \texttt{append} function:

\begin{lstlisting}
{-@ (++) :: xs:[a] -> ys:[a] -> { zs:[a] | length zs == length xs + length ys } @-}
(++) :: [a] -> [a] -> [a]
[] ++ ys = ys
(x:xs) ++ ys = x:(xs ++ ys)
\end{lstlisting}

To use non-measures in refinements (for example the \texttt{append} function above), the user has to mark them with the \texttt{\{-@ reflect @-\}} pragma and provide the proofs of the refinements themselves (see sections~\ref{sec:proof_combinators} and~\ref{sec:case_studies}).

\subsection{Proof combinators}\label{sec:proof_combinators}

To make formal proofs of Haskell code possible, LiquidHaskell also provides a set of proof combinators in the \texttt{Language.Haskell.Liquid.ProofCombinators} module. The most important one is the equality operator (\texttt{===}):

\begin{lstlisting}
infixl 3 ===
{-@ (===) :: x:a -> { y:a | y == x } -> { z:a | z == x && z == y } @-}
(===) :: a -> a -> a
_ === y = y
\end{lstlisting}
The refinement on the operator asserts that both sides are provably equal and that it returns a value that is itself equal to both inputs. The implementation ignores the first argument and just returns the second, so at runtime only the last step of the reasoning is kept.

To make proofs look nicer, LiquidHaskell defines a \texttt{Proof} type alias and a \texttt{QED} data type together with a triple star operator (\texttt{***}):
\begin{lstlisting}
type Proof = ()

infixl 3 ***
{-@ assume (***) :: a -> p:QED -> { if (isAdmit p) then false else true } @-}
(***) :: a -> QED -> Proof
_ *** _ = ()

data QED = Admit | QED

{-@ measure isAdmit :: QED -> Bool @-}
{-@ Admit :: {v:QED | isAdmit v } @-}
\end{lstlisting}

\texttt{Proof} is just an alias for the normal Haskell unit type. The \texttt{QED} type has two constructors, \texttt{Admit} if you want to abort your proof and \texttt{QED} to run it. The refinement uses syntactic sugar for proofs:

\begin{lstlisting}
{-@ {- ... -} -> { property } @-} is the same as {-@ {- ... -} -> { x:() | property } @-}
\end{lstlisting}

These combinators allow us to make the example from the introduction (section~\ref{sec:introduction}) into valid Haskell code that is checked by LiquidHaskell:

\begin{lstlisting}
{-@ x :: { fact 3 == 6 } @-}
x :: Proof
x = fact 3           -- use definition
    === 3 * fact (3 - 1) -- simplify
    === 3 * fact 2       -- use definition
    {- steps left out for clarity -}
    === 3 * 2 -- simplify
    === 6
    *** QED
\end{lstlisting}


\section{Case studies}\label{sec:case_studies}

\subsection{Deriving efficient implementations from simple specifications}\label{sec:efficient}

Often, the simplest implementation of a function is not the most efficient version, but more efficient implementations are harder to reason about. Take for example a naive \texttt{reverse} function that uses the \texttt{append} function defined earlier in section~\ref{sec:reasoning}:

\begin{lstlisting}
{-@ reverse :: xs:[a] -> { ys:[a] | length xs == length ys } @-}
reverse [] = []
reverse (x:xs) = reverse xs ++ [x]
\end{lstlisting}

Because for every element in the list it calls \texttt{append}, which in turn has to traverse the whole first argument every time, this naive version has \O{n^2} time complexity where $n$ is the length of the list. Using the combinators from LiquidHaskell for equational reasoning, we can derive a more efficient version that uses an explicit accumulator and provably has the same behavior. To be able to use the \texttt{append} and reverse function in the refinement we need to add \texttt{reflect} annotations as neither function is a measure:

\begin{lstlisting}
{-@ reflect (++) @-}
{-@ reflect reverse @-}

{-@ revApp :: xs:[a] -> ys:[a] -> { zs:[a] | revApp xs ys == reverse xs ++ ys } @-}
revApp :: [a] -> [a] -> [a]
revApp xs ys = reverse xs ++ ys
\end{lstlisting}

The first step in the actual implementation is to pattern match on the first argument and replace the variable with the matched result:

\begin{lstlisting}
{-@ revApp :: xs:[a] -> ys:[a] -> { zs:[a] | revApp xs ys == reverse xs ++ ys } @-}
revApp :: [a] -> [a] -> [a]
revApp [] ys = reverse [] ++ ys
revApp (x:xs) ys = reverse (x:xs) ++ ys
\end{lstlisting}

With that, we can use LiquidHaskell's equational reasoning combinator (\texttt{===}) to simplify both equations by replacing the calls to \texttt{reverse} with its definition:

\begin{lstlisting}
{-@ revApp :: xs:[a] -> ys:[a] -> { zs:[a] | revApp xs ys == reverse xs ++ ys } @-}
revApp :: [a] -> [a] -> [a]
revApp [] ys = reverse [] ++ ys
    === [] ++ ys
    === ys
revApp (x:xs) ys = reverse (x:xs) ++ ys
    === (reverse xs ++ [x]) ++ ys
\end{lstlisting}

At this point, the first equation is already fully simplified, but we can not simplify the second equation further. The problem is that we have not proven that append is associative yet, so we can not move the parenthesis at the moment. We can prove this fact separately using the \texttt{?} combinator to apply the induction hypothesis:

\begin{lstlisting}
{-@ apAssocP :: xs:[a] -> ys:[a] -> zs:[a] -> { (xs ++ ys) ++ zs == xs ++ (ys ++ zs) } @-}
apAssocP :: [a] -> [a] -> [a] -> Proof
apAssocP [] ys zs = ([] ++ ys) ++ zs
    === ys ++ zs
    === [] ++ (ys ++ zs)
    *** QED
apAssocP (x:xs) ys zs = ((x:xs) ++ ys) ++ zs
    === x:(xs ++ ys) ++ zs
    === x:((xs ++ ys) ++ zs)
    ? apAssocP xs ys zs
    === x:(xs ++ (ys ++ zs))
    === x:xs ++ (ys ++ zs)
    *** QED
\end{lstlisting}

With this auxiliary lemma, we can continue the definition of the optimized reverse function:

\begin{lstlisting}
{-@ revApp :: xs:[a] -> ys:[a] -> { zs:[a] | revApp xs ys == reverse xs ++ ys } @-}
revApp :: [a] -> [a] -> [a]
revApp [] ys = reverse [] ++ ys
    === [] ++ ys
    === ys
revApp (x:xs) ys = reverse (x:xs) ++ ys
    === (reverse xs ++ [x]) ++ ys
    ? apAssocP (reverse xs) [x] ys
    === reverse xs ++ ([x] ++ ys)
    === reverse xs ++ (x:[] ++ ys)
    === reverse xs ++ (x:([] ++ ys))
    === reverse xs ++ (x:ys)
\end{lstlisting}

At this point, the definition still uses the old and inefficient \texttt{reverse} function, but the definition has the form \texttt{reverse as ++ bs} just like in our specification, so we can apply this equality:

\begin{lstlisting}
{-@ revApp :: xs:[a] -> ys:[a] -> { zs:[a] | revApp xs ys == reverse xs ++ ys } @-}
revApp :: [a] -> [a] -> [a]
revApp [] ys = reverse [] ++ ys
    === [] ++ ys
    === ys
revApp (x:xs) ys = reverse (x:xs) ++ ys
    === (reverse xs ++ [x]) ++ ys
    ? apAssocP (reverse xs) [x] ys
    === reverse xs ++ ([x] ++ ys)
    === reverse xs ++ (x:[] ++ ys)
    === reverse xs ++ (x:([] ++ ys))
    === reverse xs ++ (x:ys)
    === revApp xs (x:ys)
\end{lstlisting}

Now, our \texttt{revApp} does not use any other functions and only traverses the list once, making it \O{n}. Using this more generalized \texttt{reverse} function that also appends the second argument, we can define a third version that only takes a single argument and behaves like the first version:

\begin{lstlisting}
{-@ reverse' :: xs:[a] ->
    { ys:[a] | ys == reverse xs } @-}
reverse' :: [a] -> [a]
reverse' xs = reverse xs
    === undefined -- step missing
    === reverse xs ++ []
    === revApp xs []
\end{lstlisting}

Using the refinement of \texttt{revApp} we can derive the last step from the step before, but we are still unable to complete the definition. LiquidHaskell will not allow to just add an empty list to the right. We have to prove the fact that the first step and this step are the same with a right identity proof:

\begin{lstlisting}
{-@ rightIdP :: xs:[a] -> { xs ++ [] == xs } @-}
rightIdP :: [a] -> Proof
rightIdP [] = [] ++ []
    === []
    *** QED
rightIdP (x:xs) = (x:xs) ++ []
    === x:(xs ++ [])
    ? rightIdP xs
    === x:xs
    *** QED
\end{lstlisting}

With this lemma we can complete the definition of the new reverse function:

\begin{lstlisting}
{-@ reverse' :: xs:[a] -> { ys:[a] | ys == reverse xs } @-}
reverse' :: [a] -> [a]
reverse' xs = reverse xs
    ? rightIdP (reverse xs)
    === reverse xs ++ []
    === revApp xs []
\end{lstlisting}

At this point we could remove all the reasoning steps, simplifying the function to just the result. However given how the \texttt{===} operator is defined to ignore its first argument and return the second argument, this will be optimized away by the compiler, so we can leave in the explicit equality proof of the two functions.

\subsection{Proving amortized complexity of data structures}\label{sec:complexity}

The previous case study improved the complexity of \texttt{reverse} from \O{n^2} to \O{n}, but focused on showing that both implementations behave the same way. Our second case study thus focuses on proving complexity itself. In particular, We show the proof of the proof of the amortized complexity of a stack with a \textit{multipop} operation. In particular, we proof the amortized complexity in a \textit{linear} setting, this means that intermediate data structures are not reused.

Sometimes when analyzing the complexity of operations on data structures, the worst case is too pessimistic. One of the simplest examples is a stack that has two operations: \texttt{push} adds one cell on top of the stack and is in \O{1}, \texttt{multipop(k)} removes the top $k$ elements from the stack. In the worst case $k$ is the height of the stack, so \texttt{multipop} is in \O{n}. But in order to be able to remove an element from the stack, it needs to have been pushed at some point. We can use this to "spread out" the cost of \texttt{multipop} over multiple \texttt{push} operations. Such a stack can be defined in Haskell as follows. Normally \texttt{multipop} would return the removed elements along with the rest of the stack, but this just unnecessarily clutters the proofs later:

\begin{lstlisting}
data Stack a = Empty | Elem a (Stack a)

{-@ reflect push @-}
push :: a -> Stack a -> Stack a
push x s = Elem x s

{-@ reflect multipop @-}
{-@ multipop :: Nat -> Stack a -> Stack a @-}
multipop :: Int -> Stack a -> Stack a
multipop _ Empty = Empty
multipop 0 s = s
multipop n (Elem _ s) = multipop (n - 1) s
\end{lstlisting}

To formally prove the complexity, we will use the \textit{physicist's method}. It requires us to define a potential function $\Phi$ ("phi") that takes a state of the data structure and returns some cost. This costs represents how much time units need to be saved up for expensive operations for a given state of the data structure. It has to be zero for an empty data structure and greater or equal to zero for all other states. The \textit{amortized cost} is then defined as $c + \Phi(h') - \Phi(h)$ where $c$ is the actual time cost of the operation, $h$ is the data structure before the operation and $h'$ is the data structure afterwards. Looking at a chain of operations $1, 2, ..., n$, the sum of the amortized time is an overestimation of the actual cost by $\Phi(h_n)$ and thus an upper bound on the actual cost (see equation~\ref{eq:sum}, remember that by definition $\Phi(h_0) = 0$ and $\Phi(h_i) \ge 0$).

\begin{gather}
\nonumber (c_1 + \cancel{\Phi(h_1)} - \Phi(h_0)) + (c_2 + \cancel{\Phi(h_2)} - \cancel{\Phi(h_1)}) + ... + (c_n + \Phi(h_n) - \cancel{\Phi(h_{n - 1})}) = \\
  c_1 + c_2 + ... + c_n + \Phi(h_n) - \Phi(h_0) \label{eq:sum}
\end{gather}

To get the actual cost of an operation we additionally have to define a timing function \texttt{multipopT} that returns $1$ for non-recursive cases:

\begin{lstlisting}
{-@ measure phi @-}
{-@ phi :: Stack a -> Nat @-}
phi :: Stack a -> Int
phi Empty = 0
phi (Elem _ s) = 1 + phi s

{-@ reflect multipopT @-}
{-@ multipopT :: Nat -> Stack a -> Nat @-}
multipopT :: Int -> Stack a -> Int
multipopT _ Empty = 1
multipopT 0 _ = 1
multipopT n (Elem _ s) = 1 + multipopT (n - 1) s
\end{lstlisting}

For the actual proof we will start with the two base cases of \texttt{multipop}. Our proof goal is to keep the amortized cost below a fixed constant. LiquidHaskell does not have an existential quantifier (see section~\ref{sec:hood}), so we have to guess a constant beforehand. For this proof we will choose $2$:

\begin{lstlisting}
{-@ multipopP :: k:Nat -> s:Stack a ->
    { multipopT k s + phi (multipop k s) - phi s <= 2 } @-}
multipopP :: Int -> Stack a -> Proof
multipopP 0 s = multipopT 0 s
    + phi (multipop 0 s) - phi s
-- Use definition of multipop/T 0 _
  === 1 + phi s - phi s
-- phi s cancels, 1 <= 2
  *** QED
multipopP k Empty = multipopT k Empty
    + phi (multipop k Empty) - phi Empty
-- Use definition of multipop/T _ Empty
  === 1 + phi Empty - phi Empty
-- phi Empty cancels, 1 <= 2
  *** QED
\end{lstlisting}

To avoid cluttering the proof of the recursive case, we use an \textit{as pattern} to alias \texttt{Elem x s} to \texttt{xs}. The first steps are again applying the definitions of the used functions and simplifying the result. In the end we use the \texttt{?} operator to add the induction hypothesis to the proof step and thus complete the proof:

\begin{lstlisting}
multipopP k xs@(Elem x s) = multipopT k xs
    + phi (multipop k xs) - phi xs
-- Use definition of multipopT for recursive case
  === (1 + multipopT (k - 1) s)
    + phi (multipop k xs) - phi xs
-- Use definition of multipop for recursive case
  === (1 + multipopT (k - 1) s)
    + phi (multipop (k - 1) s) - phi xs
-- Use definition of phi (Elem _ _)
  === (1 + multipopT (k - 1) s)
    + phi (multipop (k - 1) s) - (1 + phi s)
-- Remove parentheses
  === 1 + multipopT (k - 1) s
    + phi (multipop (k - 1) s) - 1 - phi s
-- 1s cancel out
  === multipopT (k - 1) s
    + phi (multipop (k - 1) s) - phi s
-- use induction hypothesis
  ? multipopP (k - 1) s
  *** QED
\end{lstlisting}

Because we were successful in bounding the amortized complexity with a constant, our amortized runtime of \texttt{multipop} is in \O{1}.

This technique can also be used for much more sophisticated data structures. In "Liquid Amortization - Proving amortized complexity with LiquidHaskell"~\cite{amortized} we prove the amortized complexity of finger trees with this method.

\section{LiquidHaskell under the hood}\label{sec:hood}

To check all the refinement types, LiquidHaskell uses a SMT (Satisfiability Modulo Theories) solver, by default the z3 solver~\cite{z3}. Contrary to SAT (satisfiability) solvers, which can only solve boolean logic, SMT solvers can additionally reason about builtin theories, for example linear arithmetic. So, similar to how SAT solvers would try to find an assignment of true or false to the variables in a formula like this:

\begin{equation}
(x_1 \vee \neg x_2) \wedge (\neg x_1 \vee x_2 \vee x_3) \wedge \neg x_1
\end{equation}

An SMT solver finds valid assignments to formulas like:

\begin{equation}
\forall x\ y\ z.\ 3x + 2y - z > 4
\end{equation}

Using the SMT-LIB 2 standard~\cite{smt-lib} the formula above can be encoded for the z3 solver like this (note that functions that take unit as argument are regarded as constants):

\begin{lstlisting}
(declare-fun x () Int)
(declare-fun y () Int)
(declare-fun z () Int)

(assert (> (- (+ (* 3 x) (* 2 y)) z) 4))
(check-sat)
(get-model)
\end{lstlisting}

The solver then finds a valid assignment, in this case $x = 2$, $y = 0$, $z = 0$. To check the refinements, LiquidHaskell encodes them in the SMT-lib standard as assertions and hands them over to z3.

With a trivial refinement type like this:
\begin{lstlisting}
{-@ simple :: { x:Int | x > 2 } -> { x > 0 } @-}
simple :: Int -> Proof
simple _ = ()
\end{lstlisting}

LiquidHaskell generates the following SMT-LIB 2 code, simplified here. Note that LiquidHaskell negates the actual goal and checks if the solver returns \texttt{unsat}:

\begin{lstlisting}
(declare-fun x () Int)

(assert (> x 2))
(push 1)
(assert (not (> x 0)))
(check-sat)
; SMT Says: Unsat
(pop 1)
(exit)
\end{lstlisting}

As the SMT solver only has builtin theories about linear arithmetic, it can only reason about equality for other types like \texttt{String}.

For arbitrary formulas, the runtime of the solver is unbounded (see~\cite{lh_quantifiers}). This would be a very poor user experience for a type checker, where small changes in the types could cause the type checker to run forever. For this reason, LiquidHaskell only uses forall quantifiers and those only at the outermost level. There are no existential quantifiers and no higher rank types. For the proofs, this means that sometimes we need to guess constants instead of quantifying over them. On the other hand, all refinements are guaranteed to terminate eventually. While the runtime can still be very high, the restrictions force the user to split refinement more often, so usually the proofs terminate quickly.

\section{Limitations}\label{sec:limitations}

\subsection{Combinatorial explosion of constraints}

For functions with deep pattern matches, LiquidHaskell has to generate all other possible patterns for the constraints which leads to a combinatorial explosion of constraints. For simple functions, like the ones described in section~\ref{sec:case_studies}, LiquidHaskell generates less than 100 constraints for the SMT solver. But, for example, to prove the termination of the concatenation of two finger trees, it has to generate over 3000. This makes type checking take several minutes for a rather small source file (see~\cite{amortized}).

\subsection{Error messages}

Particularly bad are also the error messages at the moment. If there is a type mismatch and the refinement is rejected, LiquidHaskell shows the expected type and the actual type with all their refinements and mentions that the two are not in a subtype relation. Usually the types are so convoluted that the only helpful thing in the message is the line number where the error happened.

Even worse are errors regarding totality. If the user forgets a pattern match that is not excluded by the preconditions, LiquidHaskell displays a hard coded error message mentioning \texttt{Addr\#}. This apparently stems from the recent conversion of LiquidHaskell to a GHC plugin (see section~\ref{sec:conclusion}).

\subsection{Missing support for type classes}

At the time of writing there are some limitations to the usefulness of LiquidHaskell. First and foremost, the missing support for type classes. Most of the standard Haskell type classes have laws in addition to their methods. These laws are only stated in the documentation, but never checked by a compiler. For example, the \texttt{Semigroup} type class specifies that the combine operation (\texttt{<>}) has to be associative:

\begin{lstlisting}
(a <> b) <> c == a <> (b <> c)
\end{lstlisting}

As seen in section~\ref{sec:efficient}, such properties are often needed during proofs. One could imagine a type class \texttt{VSemigroup} (for \textit{verified} semigroup) that looks like this:

\begin{lstlisting}
class Semigroup a => VSemigroup a where
    {-@ assocP :: x:a -> y:a -> z:a ->
        { (x <> y) <> z == x <> (y <> z) } @-}
    assocP :: a -> a -> a -> Proof
\end{lstlisting}

This general form is however not allowed. The only form of type classes LiquidHaskell accepts are so called \textit{measure classes}, that share the limitations of normal measures (see section~\ref{sec:reasoning}) and thus do not allow for methods with more than one argument.

\section{Comparison with other proof assistants}\label{sec:comparison}

\subsection{Reasoning about Haskell code}

The strongest advantage of LiquidHaskell over external theorem provers like Coq~\cite{coq}, Lean~\cite{lean} or Isabelle/HOL~\cite{isabelle} is that the proofs are done directly in Haskell itself. While there are tools to automatically translate Haskell to Coq (\texttt{hs-to-coq}~\cite{hs-to-coq}) or Isabelle/HOL (\texttt{Haskabelle}~\cite{haskabelle}, only for Isabelle2009), they do not support the full range of Haskell's expressiveness. For example Haskell supports polymorphic recursion in data types which is not directly representable in the language of Isabelle/HOL (see~\cite{amortized} for a practical use of polymorphic recursion).

\subsection{Equational reasoning}

One advantage of the limited automation of LiquidHaskell is that it forces the user to explicitly provide their reasoning steps as part of the proof code. Usually, theorem provers like Coq or Lean use so called \textit{apply style} where the user repeatedly applies \textit{tactics} to prove the goal. Without inspecting the proof state after each tactic, it is not really possible to follow more complicated proofs as a reader. For example, even the rather simple associativity proof of list append in Coq looks like this:

\begin{lstlisting}
Theorem app_assoc : forall l1 l2 l3 : natlist,
    (l1 ++ l2) ++ l3 = l1 ++ (l2 ++ l3).
Proof.
    intros l1 l2 l3.
    induction l1 as [| n l1' IHl1'].
    - reflexivity.
    - simpl. rewrite -> IHl1'. reflexivity.
Qed.
\end{lstlisting}

Only Isabelle/HOL provides a similar means to do true equational reasoning in the form of Isar~\cite{isar}. In Isar, the append proof looks very similar to the LiquidHaskell version, including the explicit use of the induction hypothesis:

\begin{lstlisting}
lemma app_assoc:
    "(xs ++ ys) ++ zs = xs ++ (ys ++ zs)"
proof(induction xs)
    case Nil
    have "([] ++ ys) ++ zs = ys ++ zs" by simp
    also have "... = [] ++ (ys ++ zs)" by simp
    finally show ?case.
next
    case (Cons x xs)
    have "(x:xs ++ ys) ++ zs = x:(xs ++ ys) ++ zs" by simp
    also have "... = x:xs ++ (ys ++ zs)" by (simp add: Cons.IH)
    also have"... = (x:xs) ++ (ys ++ zs)" by simp
    finally show ?case.
qed
\end{lstlisting}

\section{Conclusion}\label{sec:conclusion}

Despite these limitations, LiquidHaskell is a very powerful tool. Even if the user does not care about proofs, the pre- and postconditions for functions drastically improve type safety. By default it disallows partial functions, so the number of runtime errors that can arise is substantially lower.

Additionally the limitations are not fundamental and can be fixed. In fact, the user experience of LiquidHaskell was greatly improved recently by converting it from a standalone executable to a GHC plugin~\cite{lh_plugin}. This makes it easy to install in a project by adding it to the project configuration and it also directly integrates with the Haskell language server that supplies IDE-like features to a wide range of text editors.


\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\appendix

\end{document}
\endinput
